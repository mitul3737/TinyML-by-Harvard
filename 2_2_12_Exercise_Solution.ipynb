{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC38oAH-J61Z"
      },
      "source": [
        "# Exploring DNN learning with TensorFlow\n",
        "\n",
        "In this assignment we'll dive a little deeper with a series of hands on exercises to better understand DNN learning with Tensorflow. Remember that if you are taking the class for a certificate we will be asking you questions about the assignment in the test!\n",
        "\n",
        "We start by setting up the problem for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M8CsXWUbJ61e"
      },
      "outputs": [],
      "source": [
        "# Load libraries\n",
        "import sys\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Uo1mZjY1J61f"
      },
      "outputs": [],
      "source": [
        "# This script requires TensorFlow 2 and Python 3.\n",
        "if sys.version_info.major < 3:\n",
        "    raise Exception((f\"The script is developed and tested for Python 3. \"\n",
        "                     f\"Current version: {sys.version_info.major}\"))\n",
        "\n",
        "if tf.__version__.split('.')[0] != '2':\n",
        "    raise Exception((f\"The script is developed and tested for tensorflow 2. \"\n",
        "                     f\"Current version: {tf.__version__}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3KzJyjv3rnA",
        "outputId": "2dbe9b58-bc27-4f49-ef8d-b26d3ef111e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load in fashion MNIST\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Define the base model\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G-HIniEJ61g"
      },
      "source": [
        "Neural Networks learn the best when the data is scaled / normalized to fall in a constant range. One practitioners often use is the range [0,1]. How might you do this to the training and test images used here?\n",
        "\n",
        "*A hint: these images are saved in the standard [RGB](https://www.rapidtables.com/web/color/RGB_Color.html) format*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T__JoM2eJ61h"
      },
      "outputs": [],
      "source": [
        "training_images  = training_images / 255.0\n",
        "\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "\"\"\"This is because the RGB data format uses values in the range [0,255] to represent each color and so dividing the color values by 255 will result in the data falling in the range [0,1]. Note that we need to make sure to use floating point division or the number will always be either 0 or 1 instead of in the range [0,1].\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT8M2kElJ61h"
      },
      "source": [
        "Using these improved images lets compile our model using an adaptive optimizer to learn faster and a categorical loss function to differentiate between the the various classes we are trying to classify. Since this is a very simple dataset we will only train for 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gBIpILCzJ61i",
        "outputId": "b6c3d576-61ed-4446-fb67-9ac68a6d4c8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 16s 8ms/step - loss: 0.8758 - accuracy: 0.7074\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.5496 - accuracy: 0.8043\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.4888 - accuracy: 0.8274\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.4577 - accuracy: 0.8379\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.4359 - accuracy: 0.8458\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.4583 - accuracy: 0.8345\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.45827504992485046, 0.8345000147819519]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# compile the model\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# fit the model to the training data\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "# test the model on the test data\n",
        "model.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JJMsvSB-1UY"
      },
      "source": [
        "Once it's done training -- you should see an accuracy value at the end of the final epoch. It might look something like 0.8648. This tells you that your neural network is about 86% accurate in classifying the training data. I.E., it figured out a pattern match between the image and the labels that worked 86% of the time. But how would it work with unseen data? That's why we have the test images. We can call ```model.evaluate```, and pass in the two sets, and it will report back the loss for each. This should reach about .8747 or thereabouts, showing about 87% accuracy. Not Bad!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rquQqIx4AaGR"
      },
      "source": [
        "But what did it actually learn? If we inference on the model using ```model.predict``` we get out the following list of values. **What does it represent?**\n",
        "\n",
        "*A hint: trying running ```print(test_labels[0])```*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyEIki0z_hAD",
        "outputId": "d3735bc9-bc46-4cd6-8a17-77890ac97799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step\n",
            "[6.6821877e-07 4.4758433e-08 4.2133479e-06 4.5932243e-06 1.1792221e-05\n",
            " 1.1232433e-01 1.3825409e-05 3.6249653e-01 4.3186136e-03 5.2082545e-01]\n"
          ]
        }
      ],
      "source": [
        "classifications = model.predict(test_images)\n",
        "print(classifications[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgQSIfDSOWv6"
      },
      "source": [
        "Let's now look at the layers in your model. What happens if you double the number of neurons in the dense layer. What different results do you get for loss, training time etc? Why do you think that's the case?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSZSwV5UObQP",
        "outputId": "466c96f0-df15-4d97-9d0a-44452c6da6d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.8028 - accuracy: 0.7297\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.5152 - accuracy: 0.8170\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4620 - accuracy: 0.8360\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4336 - accuracy: 0.8464\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 22s 12ms/step - loss: 0.4147 - accuracy: 0.8528\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.4394 - accuracy: 0.8442\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.43938037753105164, 0.8442000150680542]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "NUMBER_OF_NEURONS = 1024\n",
        "\n",
        "# define the new model\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                                    tf.keras.layers.Dense(NUMBER_OF_NEURONS, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "# compile fit and evaluate the model again\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "model.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0lF5MuvSuZF"
      },
      "source": [
        "Consider the effects of additional layers in the network instead of simply more neurons to the same layer. First update the model to add an additional dense layer into the model between the two existing Dense layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PQsFssC2J61k"
      },
      "outputs": [],
      "source": [
        "YOUR_NEW_LAYER = tf.keras.layers.Dense(512, activation=tf.nn.relu)  # adding 512 to this layer instead of 1024 neuron in the second layer\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "                                    YOUR_NEW_LAYER,\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "\"\"\"When training your new model if you also used a large number of Neurons in your new layer, e.g., 512, you should find that training accuracy improves in a similar manner to doubling the number of Neurons in the single layer. If you are lucky it might even be better! If you chose a small number of Neurons you may have seen no effect. This is simply an artifact of the particular model structure and dataset we are using. Again, remember there is no one size fits all solution in Neural Network model design.'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3sCcIO9J61k"
      },
      "source": [
        "Lets then compile, fit, and evaluate our model. What happens to the error? How does this compare to the original model and the model with double the number of neurons?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "b1YPa6UhS8Es",
        "outputId": "5368e38a-f1e2-4b58-eb30-7a907e9cb8bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 20s 10ms/step - loss: 0.6963 - accuracy: 0.7437\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 21s 11ms/step - loss: 0.4689 - accuracy: 0.8299\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 21s 11ms/step - loss: 0.4149 - accuracy: 0.8481\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 21s 11ms/step - loss: 0.3837 - accuracy: 0.8602\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.3589 - accuracy: 0.8692\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.4043 - accuracy: 0.8485\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4043135643005371, 0.8485000133514404]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# compile fit and evaluate the model again\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "model.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS3vVkOgCDGZ"
      },
      "source": [
        "Before you trained, you normalized the data. What would be the impact of removing that? To see it for yourself fill in the following lines of code to get a non-normalized set of data and then re-fit and evaluate the model using this data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JDqNAqrpCNg0",
        "outputId": "82c0df28-b42b-4453-f364-e12cd8829f86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 22s 12ms/step - loss: 0.4793 - accuracy: 0.8281\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.3638 - accuracy: 0.8661\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.3266 - accuracy: 0.8791\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 20s 11ms/step - loss: 0.3026 - accuracy: 0.8877\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.2850 - accuracy: 0.8937\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3651 - accuracy: 0.8755\n",
            "313/313 [==============================] - 1s 3ms/step\n"
          ]
        }
      ],
      "source": [
        "# get new non-normalized mnist data\n",
        "training_images_non = training_images * 255\n",
        "\n",
        "test_images_non = test_images * 255\n",
        "\n",
        "# re-compile, re-fit and re-evaluate\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "                                    YOUR_NEW_LAYER,\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(training_images_non, training_labels, epochs=5)\n",
        "model.evaluate(test_images_non, test_labels)\n",
        "classifications = model.predict(test_images_non)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7W2PT66ZBHQ"
      },
      "source": [
        "Sometimes if you set the training for too many epochs you may find that training stops improving and you wish you could quit early. Good news, you can! TensorFlow has a function called ```Callbacks``` which can check the results from each epoch. Modify this callback function to make sure it exits training early but not before reaching at least the second epoch!\n",
        "\n",
        "*A hint: logs.get(METRIC_NAME) will return the value of METRIC_NAME at the current step*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pkaEHHgqZbYv",
        "outputId": "8919714c-48ef-4a6a-98bd-fc84d2571522",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 20s 10ms/step - loss: 0.6544 - accuracy: 0.7618\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 21s 11ms/step - loss: 0.4469 - accuracy: 0.8374\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.4017 - accuracy: 0.8533\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 21s 11ms/step - loss: 0.3727 - accuracy: 0.8629\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e15afd31b40>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "\"\"\"Finally we added a custom callback function to help us exit training early if we reached some goal. In this case to exit early but only upon reaching at least the 2nd epoch one thing we could do is exit once the training accuracy is over 86% (as in all of our training runs we got as high as an 83.5% accuracy on the first epoch). One way to do that would be:\"\"\"\n",
        "\n",
        "\n",
        "# define and instantiate your custom Callback\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.86 ):\n",
        "      self.model.stop_training = True\n",
        "callbacks = myCallback()\n",
        "\n",
        "# re-compile, re-fit and re-evaluate\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                            tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "                            YOUR_NEW_LAYER,\n",
        "                            tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "      loss = 'sparse_categorical_crossentropy',\n",
        "      metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2-2-12-Exercise.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}